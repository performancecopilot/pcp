#!/usr/bin/env pmpython
'''
Performance Metrics Domain Agent exporting Prometheus endpoint metrics.
'''
#
# Copyright (c) 2017 Ronak Jain.
# Copyright (c) 2017 Red Hat, Inc.
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
# or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
# for more details.
#

import cpmapi as c_api
import cpmda
from pcp.pmapi import pmUnits, pmContext, pmErr
from pcp.pmda import PMDA, pmdaMetric, pmdaIndom, pmdaInstid

import argparse
from collections import OrderedDict
from ctypes import c_int
import copy
import errno
import math
import json
import os
import re
import sys
import time
import pickle
import requests
import threading


# Number of seconds to wait between poll attempts on a source that
# we've never been able to connect to & collect a list of metrics from.
empty_source_pmns_poll = 10.0

MAX_CLUSTER = 0xfff    # ~ max. number of prometheus sources
MAX_METRIC = 0x3ff     # ~ max. number of metrics per source
MAX_INDOM = 0x7fffffff # coincidentally, ~ product of above

# These numbers are combined to create uniqe numbers for several
# purposes.  The first two types are used only for internal
# pmdaprometheus purposes, and are not visible as indoms to pmapi
# clients.
#
# indom# 0: maps source nicknames to 12-bit "cluster" numbers, cluster#0 is not used
# indom# 1..4095: for each source=cluster, map metric names to pmid numbers
# indom# 4096 + cluster#*1024 ...+1023:  actual pmns indom for each metric


class Metric(object):
        ''' Metric information class '''
        def __init__(self,source,name,metricnum,instances,pcpline,helpline,typeline):
                self.source = source
                self.name = name
                self.metricnum = metricnum # seen during fetch callbacks
                self.pmid = source.pmda.pmid(source.cluster,metricnum) # add domain/cluster#
                self.indom_number = (MAX_CLUSTER+1 +
                                     (source.cluster * (MAX_METRIC+1)) +
                                     metricnum)
                self.pcpline = pcpline
                self.typeline = typeline
                self.values = {} # instance-vector-to-value
                self.assign_metadata()
                if instances:
                        self.mindom = self.source.pmda.indom(self.indom_number) # add domain#
                        self.indom_table = PersistentNameTable(self.source.pmda,
                                                               self.indom_number, MAX_INDOM)
                else:
                        self.mindom = c_api.PM_INDOM_NULL
                        self.indom_table = None
                self.obj = pmdaMetric(self.pmid,self.mtype,self.mindom,self.msem,self.munits)
                if helpline: # it could be None!
                        self.description = helpline # XXX unescape
                else:
                        self.description = ''

                try:
                        self.source.pmda.add_metric(self.mname, self.obj, self.description)
                        self.source.pmda.set_need_refresh()
                except Exception as e:
                        self.source.pmda.log("Cannot add metric %s (%d): %s" % (self.mname, self.pmid, e))
                self.source.pmda.debug("created metric %#x (%s)" % (self.pmid, self.mname))


        def assign_metadata(self):
                ''' Compute metric metadata self.{mtype,mname,msem,munits}
                from the available information already stored in self.
                '''
                self.mtype = c_api.PM_TYPE_DOUBLE # NB: allow overriding sometime

                # Split the prometheus metric name by "_", to help
                # decode convention for squishing unit/scale data.
                pieces = self.name.split('_')
                
                # XXX: pcpline semantics
                if (self.typeline == 'counter'
                    or 'total' in pieces or 'count' in pieces or 'sum' in pieces):
                        self.msem = c_api.PM_SEM_COUNTER
                else:
                        self.msem = c_api.PM_SEM_INSTANT

                if self.pcpline:
                        self.mname = 'prometheus.' + self.source.name + '.' + self.pcpline # XXX: split once it carries sem/units
                else:
                        self.mname = 'prometheus.' + self.source.name + '.' + self.name.replace(":",".")
                        
                # XXX: pcpline units
                if ('seconds' in pieces):
                        self.munits = pmUnits(0,1,0,0,3,0)
                elif ('microseconds' in pieces): # not listed in https://prometheus.io/docs/practices/naming/
                        self.munits = pmUnits(0,1,0,0,1,0)
                elif ('bytes' in pieces):
                        self.munits = pmUnits(1,0,0,1,0,0)
                else:
                        self.munits = pmUnits(0,0,0,0,0,0) # default, dimensionless old-school count

                assert self.munits and self.msem and self.mname and self.mtype

                
        def clear_values(self):
                ''' Erase all stored instance/value pairs, in anticipation of a new set. '''
                self.values.clear()
        
        def format_prom_instname(self,labels):
                ''' Format the argument labels dict as a legal PCP external instance name.
                    This is a comma separated list of name:'value' pairs where each value
                    is 'quoted'. The instance name is prefixed with a persistent unique
                    inst number - needed because the quoted value strings can contain spaces
                    and pmLookupIndom(3) only matches characters up to the first space.
                    The label dict items list is first sorted because prometheus labelsets
                    are commutative .. i.e. order of name=val pairs doesn't matter.
                '''
                name = ""
                for key,val in sorted(labels.items()):
                        if len(name) > 0:
                                name += ','
                        name += key + ":'" + val + "'"
                inst = self.indom_table.intern_lookup_value(name,True)
                return inst, str(inst) + " " + name

        def store_inst(self,labels,value):
                ''' Store given new instance/value pair. '''
                
                assert (labels is None) == (self.indom_table is None) # no metric indom flipflop
                if labels is None:
                        self.values[c_api.PM_IN_NULL] = float(value)
                else:
                        if self.pcpline: # pmwebd
                                # NB: no quoting/transforms - preserve incoming value verbatim
                                instname = labels['instance']
                                inst = self.indom_table.intern_lookup_value(instname)
                        else:
                                inst,instname = self.format_prom_instname(labels)
                        # self.source.pmda.debug('store inst=%d instname="%s" value="%s"' % (inst, instname, value))
                        self.values[inst] = float(value)

        def save(self):
                self.indom_table.save()
                        
        def fetch_inst(self,inst): # fetch instance
                if not self.values:
                        # Metric may have disappeared
                        return [c_api.PM_ERR_AGAIN, 0]
                if inst in self.values:
                        return [self.values[inst], 1]
                else:
                        return [c_api.PM_ERR_INST, 0]

                
class PersistentNameTable(object):
        '''Persistent name table.  Answers name-to-number queries by assigning
        persistent ids.  Updates pmda's indom table intermittently.
        Persistent by saving dictionary in $PCP_VAR_DIR/pmdas/,
        similarly to how pmdaCache functions do.
        '''
 
        def __init__(self,pmda,indom,maxnum):
                self.pmda = pmda
                self.indom = indom
                self.maxnum = maxnum
                self.need_save = False
                self.store_file_name = '%s/config/pmda/%d.%d.py' % (os.environ['PCP_VAR_DIR'],
                                                                    pmda.domain, indom)
                try: # slightly used!
                        with open(self.store_file_name,'rb') as f:
                                self.instances = pickle.load(f,encoding="bytes")
                                self.pmda.debug("loaded %s" % self.store_file_name)
                                self.need_save = True # to push values down into c pmda layer
                except Exception as e: # new!
                        self.instances = [] # won't be saved till nonempty

                self.names_to_instances = {}
                for i,n in enumerate(self.instances):
                        self.names_to_instances[n] = i
                # prime the pmda indom table with an empty list; the pmdaIndom
                # object will be replaced shortly within .replace_indom()
                self.pmdaindom = pmdaIndom(pmda.indom(self.indom), [])
                self.pmda.add_indom(self.pmdaindom)
                self.save() # replace it with real data now, if non-empty
                
        def save(self):
                '''Push values to the PMDA layer as well as to the backing store file.'''
                if self.need_save:
                        # save to the pmda C layer
                        indom_array = [] # pmdaInstid array, computed on demand
                        for i,n in enumerate(self.instances):
                                indom_array.append(pmdaInstid(c_int(i),str(n)))
                        # NB: use replace_indom(int,[...]) overload, to pass indom_array,
                        # else it gets ignored
                        self.pmda.replace_indom(self.pmda.indom(self.indom), indom_array)
                        self.pmda.set_need_refresh()
                        # save to disk too
                        try: # slightly used!
                                with open(self.store_file_name,'wb') as f:
                                        pickle.dump(self.instances, f, protocol=0) # 0: most compatible
                                        self.pmda.debug("saved %s, %d instances" % (self.store_file_name, len(self.instances)))
                                self.need_save = False # reset only on success
                        except Exception as e:
                                self.pmda.log("cannot save %s: %s" % (self.store_file_name, e))
                        

        def intern_lookup_value(self, name, prefix=False):
                '''Add/lookup given name, return its persistent identifier.
                If @prefix is true, prepend to the instance number to the name,
                for use in generating unique-prefixed instance names.'''
                if name in self.names_to_instances: # fast path
                        # mapping the translated name (without inst prefix) to inst
                        return self.names_to_instances[name]
                else: # new name
                        num = len(self.instances)
                        if (num > self.maxnum):
                                raise ValueError('Too many (%d) different names' % num)
                        if prefix:
                                # NB: note trickery: instances[] contains prefixed names,
                                # since those are the ones that will pass to the c pmda layer;
                                # names_to_instances[] maps unprefixed names, since those are
                                # the ones used as incoming keys.
                                name_with_prefix = str(num) + " " + name
                                self.instances.append(name_with_prefix)
                                self.names_to_instances[name] = num # nb: name without prefix
                                assert self.instances[num] == name_with_prefix
                        else:
                                self.instances.append(name)
                                self.names_to_instances[name] = num
                                assert self.instances[num] == name
                        self.need_save = True
                        return num # the new inst number

        
class SampleLineParser(object):
        '''A parser for one metric [{instance}] value [timestamp] line.  A
        state machine required since we're lexing text with embedded
        quoted freeform strings with punctuation, etc., so can't
        simply do substring searching.
        '''

        def parse_metric_name_start(self,char):
                if char.isspace():
                        pass
                else:
                        self.state = self.parse_metric_name
                        self.name = char

        def parse_metric_name(self,char):
                if char == '{':
                        self.state = self.parse_label_name_start
                        self.labels = {}
                elif char.isspace():
                        self.state = self.parse_post_metric_name
                else:
                        self.name += char

        def parse_post_metric_name(self,char):
                if char.isspace():
                        pass
                elif char == '{':
                        self.state = self.parse_label_name_start
                        self.labels = {}
                else:
                        self.state = self.parse_value
                        self.value = char

        def parse_label_name_start(self,char):
                if char.isspace():
                        pass
                elif char == '}':
                        self.state = self.parse_post_labels
                else:
                        self.state = self.parse_label_name
                        self.lname = char

        def parse_label_name(self,char):
                if char.isspace():
                        self.state = self.parse_label_equals
                elif char == '=':
                        self.state = self.parse_label_value_start
                else:
                        self.lname += char

        def parse_label_equals(self,char):
                if char.isspace():
                        pass
                elif char == '=':
                        self.state = self.parse_label_value_start
                else:
                        raise ValueError("Expected =")

        def parse_label_value_start(self,char):
                if char.isspace():
                        pass
                elif char == '"':
                        self.state = self.parse_label_value
                        self.lvalue = ""
                else:
                        raise ValueError("Expected \"")

        def parse_label_value(self,char):
                if char == '\\':
                        self.state = self.parse_label_value_escapechar
                elif char == '"':
                        self.state = self.post_label_value
                        self.labels[self.lname]=self.lvalue
                        self.lname = None
                        self.lvalue = None
                else:
                        self.lvalue += char
                                              
        def parse_label_value_escapechar(self,char):
                # XXX: maybe pass through \-escaped identifiers literally through to pcp
                # otherwise we'd have to filter them out at indom-instance-name creation
                if char == '\\':
                        self.state = self.parse_label_value
                        self.lvalue += '\\'
                elif char == 'n':
                        self.state = self.parse_label_value
                        self.lvalue += '\n'
                elif char == '"':
                        self.state = self.parse_label_value
                        self.lvalue += '\"'
                else:
                        self.state = self.parse_label_value
                        self.lvalue += '\\' + char # transcribe \XYZ literally
        
        def post_label_value(self,char):
                if char.isspace():
                        pass
                elif char == ',':
                        self.state = self.parse_label_name_start
                elif char == '}':
                        self.state = self.parse_post_labels
                else:
                        raise ValueError("Expected , or }")

        def parse_post_labels(self,char):
                if char.isspace():
                        pass
                else:
                        self.state = self.parse_value
                        self.value = char
                                              
        def parse_value(self,char):
                if char.isspace(): # timestamp possibly following
                        self.state = self.parse_chomp
                else:
                        self.value += char

        def parse_chomp(self,char): # ignored stuff
                pass

        def __init__(self,line):
                # mis-initialize output variables to force state
                # machine transitions to do it right
                self.name = None
                self.value = None
                self.labels = None
                self.lname = None
                self.lvalue = None

                # run state machine
                self.state = self.parse_metric_name_start
                for char in line:
                        self.state(char)

                assert self.name
                assert self.value

                                              
class Source(object):
        '''An instance of this class represents a distinct Prometheus exporter,
        identified by a nickname (the next PMNS component beneath prometheus.*),
        and a URL.  Metrics will show up at prometheus.NICKNAME.MET.RIC as/when
        the source is online.
        '''

        def __init__(self, name, cluster, path, pmda):
                self.name = name # source nickname
                self.cluster = cluster # unique/persistent id# for nickname
                self.path = path # pathname to .url file
                self.pmda = pmda # the shared pmda

                self.refresh_time = 0 # "never"
                self.requests = requests.Session() # allow persistent connections etc.
                
                # persistently assign numeric id to our metrics
                self.pmids_table = PersistentNameTable(self.pmda, cluster, MAX_METRIC)
                
                self.metrics_by_name = {} # name -> Metric
                self.metrics_by_num = {} # number (last component of pmid) -> Metric
                

        def old_enough_for_refresh(self):
                '''But what's "old"?  Aye, there's the rub.  If it's empty (no
                metrics), then it has probably never been connected to
                successfully.  

                OTOH, if it hasn't been fetched from "recently", maybe
                it grew new metrics, and may need a look-see.  So we
                could track the last time a fetch was done to this source,
                and "time out" the PMNS from it.
                '''
                now = time.time()
                last_try_age = now - self.refresh_time
                return len(self.metrics_by_name) == 0 and last_try_age > empty_source_pmns_poll
                
        def parse_metric_line(self,line,pcpline,helpline,typeline):
                '''
                Parse the sample line, identify/create corresponding metric & instance.
                '''
                try:
                        sp = SampleLineParser(line)
                        # self.pmda.debug("parsed '%s' -> %s %s %s" % (line, sp.name, sp.labels, sp.value))
                        if sp.name in self.metrics_by_name:
                                m = self.metrics_by_name[sp.name]
                                assert self.metrics_by_num[m.metricnum] == m
                        else:
                                metricnum = self.pmids_table.intern_lookup_value(sp.name)
                                m = Metric(self,sp.name,metricnum,sp.labels,
                                           pcpline,helpline,typeline)
                                self.metrics_by_name[sp.name] = m
                                self.metrics_by_num[metricnum] = m  # not pmid!

                        m.store_inst(sp.labels,sp.value)
                except Exception as e:
                        self.pmda.log("cannot parse/store %s: %s" % (line, e))
                pass


        def parse_lines(self,text):
                '''Refresh all the metric metadata as it is found, including creating
                new metrics.  Store away metric values for subsequent
                fetch()es.  Parse errors may result in exceptions.
                That's OK, we don't try heroics to parse non-compliant
                data.  Return number of metrics extracted.
                '''

                num_metrics = 0
                lines = text.splitlines()
                pcpline = None
                helpline = None
                typeline = None
                state="metadata"
                for line in lines:
                        # self.pmda.debug("line: %s state: %s" % (line, state))
                        l = line.strip() # whitespace
                        if l == "": # blank line, ignore, no state change
                                continue
                        elif l.startswith("#"): # comment
                                if state == "metrics":
                                        state = "metadata"
                                        pcpline = None # NB: throw away previous block's metadata
                                        helpline = None
                                        typeline = None

                                lp = l.split()
                                if len(lp) < 2:
                                        continue
                                # NB: for a well-formed exporter file,
                                # the # metadata blocks must precede
                                # the metric values; we can ignore
                                # lp[2].
                                if lp[1] == 'PCP': # XXX: extend, regularize, document
                                        pcpline = ' '.join(lp[2:])
                                elif lp[1] == 'HELP':
                                        # assume lp[2] matches metric name
                                        helpline = ' '.join(lp[3:]) # XXX unescape \\ \n
                                elif lp[1] == 'TYPE':
                                        # assume lp[2] matches metric name
                                        typeline = ' '.join(lp[3:])
                                else:
                                        pass # ignore other comment lines
                        else: # metric{...} value line
                                state = "metrics"
                                # NB: could verify helpline/typeline lp[2] matches,
                                # but we don't have to go out of our way to support
                                # non-compliant exporters.
                                self.parse_metric_line(l,pcpline,helpline,typeline)
                                num_metrics += 1

                # NB: this logic only ever -adds- Metrics to a Source.  If a source
                # stops supplying some metrics, then a PCP app will see a PM_ERR_INST
                # coming back when it tries to fetch them.  We could perhaps keep the
                # set of -current- metrics fresh, i.e., track any metrics that were
                # in the Source but were not processed by any parse_metric_line() call.
                # Then we could remove the Metric, and thereby trigger a PM_ERR_PMID
                # for them.  In both cases though, we have no values.
                
                return num_metrics
                

        def refresh(self,timeout):
                '''
                Find the target URL by reading .url file.
                Fetch the target URL.
                Parse the resulting document.
                '''
                # clear cached values from all my metrics
                for n,m in self.metrics_by_name.items():
                        m.clear_values()
                # XXX: ditch metrics no longer found in document

                self.refresh_time = time.time()
                
                # (re)read the URL from given file
                try:
                        url = open(self.path,'r').read().strip()
                except Exception as e:
                        self.pmda.log("cannot read %s: %s" % (self.path, e))
                        return
                # fetch the URL
                try:
                        if url.startswith('file://'):
                                t = open(url[7:],'r').read()
                        else:
                                r = self.requests.get(url,timeout=timeout)
                                r.raise_for_status() # non-200?  ERROR
                                t = r.text
                        s = self.parse_lines(t)
                        # save metric & indom lookup tables changes, if any 
                        for n,m in self.metrics_by_name.items():
                                m.save()
                        self.pmids_table.save()
                        self.pmda.debug("fetched %d bytes with %d metrics from %s" % (len(t), s, url))
                except Exception as e:
                        self.pmda.log("cannot fetch %s: %s" % (url, e))
                        return
                
        def fetch(self, item, inst):
                ''' Retrieve metric/instance values that ought to have been found
                by a recent refresh().  PM_ERR_AGAIN signals a no go.'''
                try:
                        m = self.metrics_by_num[item]
                        return m.fetch_inst(inst)
                except Exception as e:
                        self.pmda.log("cannot fetch item %d inst %d: %s" % (item,inst,e))
                        return [c_api.PM_ERR_AGAIN, 0]


class PrometheusPMDA(PMDA):
        def __init__(self,pmda_name,domain,config,timeout):
                ''' Initialize the PMDA
                '''
                # urgent: say hi the invoking pmcd, redirect stderr etc.
                PMDA.__init__(self,pmda_name,domain)
                self.connect_pmcd()
                # now everything else may take time
                self.pmda_name = pmda_name
                self.config_dir = config
                self.config_dir_ctime = None
                self.timeout = timeout
                self.debug_flag = ('PCP_PYTHON_DEBUG' in os.environ)

                # the list of configured sources
                self.source_by_name = {}
                # persistently assign numeric id to source names -> pmid "cluster" numbers
                self.cluster_table = PersistentNameTable(self, 0, MAX_CLUSTER)
                reserved_cluster = self.cluster_table.intern_lookup_value("*reserved-source2cluster*")
                assert reserved_cluster == 0
                self.source_by_cluster = {}

                # Add a IS_DYNAMIC_ROOT metric that serves as a reminder to
                # pmcd to delegate all pmns requests to us.  Do it early, before
                # other metrics may populate beneath the prometheus.* prefix.
                dynamic_pmid = (0 << 31) | (511 << 22) | (144 << 10) | 0
                dynamic_root = pmdaMetric(dynamic_pmid,0,0,0,pmUnits())
                self.add_metric('prometheus',dynamic_root,'dynamic root for prometheus metrics')
                self.set_need_refresh()

                # used all the time, but we welcome it only for initial pmns population purposes
                self.set_refresh_metrics(self.refresh_metrics_for_pmns)
                # used during fetch
                self.set_refresh_all(self.refresh_some_clusters_for_fetch) # "all" is a misnomer
                self.set_fetch_callback(self.fetch_callback)


        def assert_source_invariants(self, name=None, cluster=None):
                '''
                Assert some invariants about the known sources
                '''
                if name:
                        s = self.source_by_name[name]
                        assert s == self.source_by_cluster[s.cluster]
                if cluster:
                        s = self.source_by_cluster[cluster]
                        assert s == self.source_by_name[s.name]

                       
        def rescan_confdir(self):
                '''Scan the configuration directory for any new .url files containing
                prometheus URLs.  Ensure there is a Source registered in
                the self.source_by_name dictionary for each one.

                If the config_dir hasn't changed lately, do nothing.
                This is important because this callback is invoked
                -all-the-time- by src/python/pmda.c.
                '''
                try:
                        ctime = os.path.getctime(self.config_dir)
                        if not self.config_dir_ctime or self.config_dir_ctime < ctime:
                                self.config_dir_ctime = ctime
                                # fall through
                        else: # no change, don't rescan directory
                                return
                except:
                        # If the stat failed, there's not much we can do.
                        # pass # could fall through
                        return
                
                nickname_regexp = re.compile(r"^[A-Za-z][A-Za-z0-9_.]*$")
                
                # XXX: nuke sources related to removed files
                self.log("walking %s, ctime %s" % (self.config_dir, self.config_dir_ctime))
                save_cluster_table = False
                for root, dirs, files in os.walk(self.config_dir):
                        for file in files:
                                # compute nickname for source:
                                # the part of the filename before .url
                                file_split = os.path.splitext(file)
                                self.debug("found %s/%s => %s" % (root, file, file_split))
                                if len(file_split) != 2 or file_split[1] != ".url":
                                        continue
                                name = file_split[0]
                                if not nickname_regexp.match(name):
                                        self.log("Bad nickname %s" % file)
                                        continue

                                if name in self.source_by_name:
                                        self.assert_source_invariants(name=name)
                                        pass
                                else:
                                        try:
                                                path = os.path.join(root, file)
                                                cluster = self.cluster_table.intern_lookup_value(name)
                                                source = Source(name, cluster, path, self)
                                                self.source_by_name[source.name] = source
                                                self.source_by_cluster[source.cluster] = source
                                                save_cluster_table = True
                                                self.log("Found source %s cluster %d" % (name, cluster))
                                        except Exception as e:
                                                self.log("Error allocating new cluster/source %s (%s)" % (name, e))
                if save_cluster_table:
                        self.cluster_table.save()


        def refresh_metrics_for_pmns(self):
                '''Refresh our list of Sources.  Then have each "old" Source do a
                fetch, so as to populate/refresh the PMNS.
                '''
                self.rescan_confdir() # get our Source list up to date
                
                # do a batch fetch of all empty sources to ensure pmns is populated
                clusters=[]
                for k,v in self.source_by_cluster.items():
                        if v.old_enough_for_refresh():
                                clusters.append(k)
                if len(clusters) > 0:
                        self.refresh_some_clusters_for_fetch(clusters)

                
        def fetch_callback(self, cluster, item, inst):
                ''' Main fetch callback which returns the value of the metric '''
                self.assert_source_invariants(cluster=cluster)
                try:
                        if cluster in self.source_by_cluster:
                                return self.source_by_cluster[cluster].fetch(item, inst)
                        else:
                                return [c_api.PM_ERR_PMID, 0]
                except Exception as e:
                        return [c_api.PM_ERR_AGAIN, 0] # was there before

        def refresh_worker(self,cluster):
                try:
                        self.assert_source_invariants(cluster=cluster)
                        self.source_by_cluster[cluster].refresh(self.timeout)
                except Exception as e:
                        self.log("Cannot refresh cluster %d: %s" % (cluster, e))

        def refresh_some_clusters_for_fetch(self,_clusters):
                '''Called once per pmFetch batch handling, before
                prometheus_fetch_callback calls.  Creates threads to
                fetch data in parallel.
                '''
                clusters = [int(l) for l in _clusters] # convert from PyLong
                threads = []

                self.debug("refreshing clusters %s" % clusters)
                for c in clusters:
                        t = threading.Thread(target=self.refresh_worker,
                                             args=(c,)) # <- important comma
                        threads.append(t)
                        ## t.daemon = True
                        t.start()
                for t in threads:
                        t.join() # XXX: timeout maybe?
                        
                # NB: this particular approach not only fetches the remote documents
                # in parallel, but also -processes- them in parallel, up to and including
                # calling the python pmda bindings from various threads.  This seems to
                # be OK.  The pmda main thread is the only one that will actually talk to
                # pmcd, based on the python shared dicts / callbacks, but that only happens
                # when all these threads are dead.  The various threads don't share any python-side data structures.
                        
        def set_need_refresh(self):
                cpmda.set_need_refresh()
                self.pmns_refresh()

        def debug(self,s):
                # XXX: conditionalize
                super(PrometheusPMDA,self).log("debug: "+s)
                # sys.stderr.write('debug:' + s + '\n')

        def log(self,s):
                super(PrometheusPMDA,self).log(s)
                # sys.stderr.write('log:' + s + '\n')
                        
                        
if __name__ == '__main__':
        parser = argparse.ArgumentParser(description='Prometheus PMDA.',
                                         formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('-c','--config',
                            type=str,
                            default=os.getenv('PCP_PMDAS_DIR')+'/prometheus/urls.d',
                            help='set URL configuration directory')
        parser.add_argument('-t','--timeout',
                            type=int,
                            default=2,
                            help='HTTP GET timeout for each end-point URL (default 2 seconds)')
        args = parser.parse_args()

        pmda = PrometheusPMDA('prometheus', 144, args.config, args.timeout)
        pmda.run()
