{
    "nodes": [
        {
            ".name": "short title of node",
	    ".description": "more detailed explanation of the issue",
	    ".pcp_metrics": "information from pcp to determine whether problem",
	    ".predicate": "test to determine if problem",
	    ".urls": [ "list of extern URLs (for example kbase articles) for addressing the issue" ],
	    ".parents": ["one or more nodes that would be checked before this node"]
        },
        {
            "name": "Start",
	    "predicate": 1,
            "pcp_metrics": ["kernel.all.load.1 minute"],
            "pcp_metrics_log": ["kernel.all.load"]
        },
        {
            "name": "CPU",
	    "description": "The speed of the CPU is limiting performance",
            "pcp_metrics": ["cpubusy.*"],
            "pcp_metrics_log": ["cpubusy" ],
            "pcp_deriveds": [ "cpubusy = 1 - rate(kernel.percpu.cpu.idle)" ],
            "pcp_predicate": { "label": "processors 85% busy",
                               "metric": "cpubusy",
                               "operator": "threshold_min",
                               "value" : 0.85 },
	    "urls": [ "https://access.redhat.com/articles/767563#cpu" ],
	    "parents": ["Start"]
        },
        {
            "name": "Storage",
	    "description": "Excessive waiting for storage",
	    "pcp_metrics": ["diskbusy.*"],
	    "pcp_metrics_log": ["diskbusy"],
            "pcp_deriveds": [ "diskbusy = rate(disk.dm.avactive)" ],
            "pcp_predicate": { "label": "disk 85% busy",
                               "metric": "diskbusy",
                               "operator": "threshold_min",
                               "value" : 0.85 },
	    "parents": ["Start"]
        },
        {
            "name": "Storage - bandwidth",
	    "description": "Saturating bandwidth of storage",
	    "pcp_metrics": [ "disk.dm.total.*" ],
	    "pcp_metrics_log": [ "disk.dm.total" ],
	    "predicate": "delta(disk.dm.total)/delta(time)",
	    "parents": ["Storage"]
        },
        {
            "name": "Storage - small blocks",
	    "description": "Excessive waiting for storage",
	    "pcp_metrics": [ "disk.dm.total.*", "disk.dm.blktotal.*"],
	    "pcp_metrics_log": [ "disk.dm.total", "disk.dm.blktotal"],
            "pcp_deriveds": [ "blkrate = delta(disk.dm.total)/delta(disk.dm.blktotal)" ],
	    "pcp_predicate": { "label": "> 64 blocks per something",
                               "metric": "blkrate",
                               "operator": "threshold_min",
                               "value" : 64 },
	    "parents": ["Storage"]
        },
        {
            "name": "Memory",
	    "description": "Running low on available memory",
	    "pcp_metrics": [ "mem.util.available", "mem.physmem" ],
	    "pcp_deriveds": [ "mem_free = mem.util.available/mem.physmem" ],
            "pcp_predicate": { "label": "< 10% available memory",
                               "operator": "threshold_max",
                               "metric": "mem_free",
                               "value": 0.10 },
	    "urls": [ "https://access.redhat.com/articles/781733" ],
	    "details": "When there is little memory available the system will need to free up space when addition memory is requested.  The memory can be freed by removed cached files, flushing files to disk, and paging sections of memory to swap on storage devices.",
	    "parents": ["Start"]
        },
        {
            "name": "Memory - swapping",
	    "description": "Not enough physical memory and data being moved out to swap space",
	    "pcp_metrics":  [ "swap.pagesout" ],
	    "pcp_deriveds":  [ "swaps = rate(swap.pagesout)" ],
            "pcp_predicate": { "label": "swapping occuring",
                               "operator": "threshold_min",
                               "metric": "swaps",
                               "value": 1.0 },
	    "urls": [ "https://access.redhat.com/solutions/666753", "https://access.redhat.com/labsinfo/rhma" ],
	    "details": "When the memory pressure is excessive the operating system will move data in memory to swap space on storage devices so that the memory can be use to store other data.  Data in swap will be moved back into memory as needed.  However, there is a cost for scanning memory for candidate data to move to swap and the cost of moving data between memory and swap space is high.",
	    "parents": ["Memory"]
        },
        {
            "name": "Memory - low system memory",
	    "description": "Not enough memory in the system resulting in data being moved between memory and storage",
	    "pcp_metrics":  [ "mem.util.free", "mem.physmem" ],
	    "pcp_metrics_log":  [ "mem.util.free", "mem.physmem" ],
	    "predicate": "",
	    "article": "",
	    "parents": ["Memory - swapping"]
        },
        {
            "name": "Memory - low NUMA node memory",
	    "description": "Not enough memory on or more NUMA nodes resulting in data being moved between memory and storage",
	    "pcp_metrics":  [ "mem.numa.util.free.*", "mem.numa.util.total.*" ],
	    "pcp_metrics_log":  [ "mem.numa.util.free.*", "mem.numa.util.total.*" ],
	    "predicate": "",
	    "article": ["https://access.redhat.com/solutions/406253", "https://access.redhat.com/solutions/465463" ],
	    "parents": ["Memory - swapping"]
        },
        {
            "name": "Memory - Low mm reclaim efficency",
	    "description": "The memory management system spending too much effort relaiming memory pages",
	    "pcp_metrics":  [ "mem.vmstat.pgsteal_direct",  "mem.vmstat.pgsteal_kswapd", "mem.vmstat.pgscan_direct",  "mem.vmstat.pgscan_kswapd" ],
	    "pcp_metrics_log":  [ "mem.vmstat.pgsteal_direct",  "mem.vmstat.pgsteal_kswapd", "mem.vmstat.pgscan_direct",  "mem.vmstat.pgscan_kswapd" ],
	    "pcp_deriveds": [ "vmeff = (mem.vmstat.pgsteal_direct+mem.vmstat.pgsteal_kswapd)/(mem.vmstat.pgscan_direct+mem.vmstat.pgscan_kswapd)"],
	    "predicate": { "label":  "< 50% scan recovery rate",
			   "operator": "threshold_max",
			   "metric": "vmeff",
			   "value": 0.50 },
	    "article": "https://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases",
	    "details": "The linux virtual memory management system has a mechanism that places pages in memory into three lists: active, inactive, and free.  When a page is initially used it is put on the active list. Over time pages on the active list may fall off the end of the active list and be added to the inactive list as possible canidates to reclaim and to reuse for other data.  The inactive list is linearly scanned for possible pages to reclaim, but pages on the inactive list may have various reasons to disqualify them from being reclaimed.  This scanning of the inactive list for candidates to reclaim requires cpu processing.  One would like the the computer system to be efficient and avoid having to scan many pages on the inactive list to find few candidates to reclaim. This problem behavior can be observed with a low %vmeff in the 'sar -B' output, a low number of pgsteal/s in relation to the sum of pgscank/s and pgscand/s.",
	    "issues": ["The various aggregate pgsteal and pgscan pcp metrics do not currently exist",
		       "The specific pgsteal and pgscan metrics provided by vmstat vary between kernels"],
	    "parents": ["Start"]
        },
        {
            "name": "Memory - huge page defragmentation",
	    "description": "The system is spending large amounts of time grouping small pages of memory together into contigious physical regions of memory",
	    "pcp_metrics": [ "mem.vmstat.thp_collapse_alloc", "mem.vmstat.thp_fault_alloc", "mem.vmstat.thp_fault_fallback" ],
	    "urls": [ "http://dl.acm.org/citation.cfm?id=2930834" ],
	    "parents": ["Memory"]
        },
        {
	    "name": "Memory - huge page fragmentation",
	    "description": "The system is splitting large regions of memory (Huge pages) into small pages",
	    "pcp_metrics": [ "mem.vmstat.thp_split" ],
	    "urls": [ "https://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases" ],
	    "details": "The Transparent Huge Page (THP) mechanism in some situations can reduce the overhead caused by TLB misses by using a individual Translation Lookaside Buffer (TLB) entry that provide a virtual to physical mapping for a 2MB of region memory rather than requiring 512 TLB entries to providing the virtual to physical memory mapping 2MB of 4096 byte pages.  Thus, more memory can be address with the name number of TLB entries, reducing the number of expensive updates to the TLB to include a virtual to physical mapping that is not currently in the TLB. TLB entries are a very limited resource, ranging from dozens to hundreds of entries. There are cases where the THP mechanism may need to split the huge pages into the smaller pages or combine smallers pages into a larger page.  This can occur when the a huge page is moved between Non-Uniform Memory Access (NUMA) nodes to rebalance memory use (*). Monitoring the Performance Copilot metric mem.vmstat.thp_split (or /proc/vmstat thp_split) would indicate when the expensive splitting of huge pages is occurring.",
	    "issues": ["The RHEL7 has mem.vmstat.thp_split (thp_split in /proc/vmstat is available but on Fedora 25 /proc/vmstat has thp_split_page and thp_split_pmd which do not match up with PCP's mem.vmstat.thp_split"],
	    "parents": ["Memory"]
        },
        {
            "name": "Network TX",
	    "description": "Amount of network trafic sent",
	    "pcp_metrics": ["network.interface.out.bytes.*", "network.interface.baudrate.*" ],
	    "pcp_metrics_log": ["network.interface.out.bytes", "network.interface.baudrate" ],
	    "pcp_deriveds": [ "network_tx_bandwidth = rate(network.interface.out.bytes)/network.interface.baudrate" ],
	    "pcp_predicate": { "label": "network tx bandwidth",
                               "metric": "network_tx_bandwidth",
                               "operator": "threshold_min",
                               "value" : 0.85 },
	    "parents": ["Start"]
        },
        {
            "name": "Network RX",
	    "description": "Amount of network trafic received",
	    "pcp_metrics": ["network.interface.in.bytes.*", "network.interface.baudrate.*" ],
	    "pcp_metrics_log": ["network.interface.in.bytes", "network.interface.baudrate" ],
	    "pcp_deriveds": [ "network_rx_bandwidth = rate(network.interface.in.bytes)/network.interface.baudrate" ],
	    "pcp_predicate": { "label": "network rx bandwidth",
                               "metric": "network_rx_bandwidth",
                               "operator": "threshold_min",
                               "value" : 0.85 },
	    "parents": ["Start"]
        },
        {
            "name": "Network TX - Saturation",
	    "description": "Network packets being dropped",
	    "pcp_metrics": ["network.interface.out.drops.*" ],
	    "pcp_metrics_log": ["network.interface.out.drops" ],
	    "pcp_deriveds": [ "network_tx_drops = rate(network.interface.out.drops)" ],
	    "pcp_predicate": { "label": "network tx drops",
                               "metric": "network_tx_drops",
                               "operator": "threshold_min",
                               "value" : 0.01 },
	    "url": [ "https://access.redhat.com/solutions/21301"],
	    "details": "Packets maybe dropped if there is not enough room in the ring buffers",
	    "issues": "The URL mentions comparing the current ring buffer size to the max allowed and increase the ring buffer size, but PCP doesn't have metrics to provide ring buffer info, a 1% packet drop threshold might be too high.",
	    "parents": ["Network TX", "Network RX"]
        },
        {
            "name": "Network RX - Saturation",
	    "description": "Network packets being dropped",
	    "pcp_metrics": ["network.interface.in.drops.*" ],
	    "pcp_metrics_log": ["network.interface.in.drops" ],
	    "pcp_deriveds": [ "network_rx_drops = rate(network.interface.in.drops)" ],
	    "pcp_predicate": { "label": "network rx drops",
                               "metric": "network_rx_drops",
                               "operator": "threshold_min",
                               "value" : 0.01 },
	    "url": [ "https://access.redhat.com/solutions/21301"],
	    "details": "Packets maybe dropped if there is not enough room in the ring buffers",
	    "issues": "The URL mentions comparing the current ring buffer size to the max allowed and increase the ring buffer size, but PCP doesn't have metrics to provide ring buffer info, a 1% packet drop threshold might be too high.",
	    "parents": ["Network RX"]
        },
        {
            "name": "Network TX - errors",
	    "description": "Show network errors",
	    "pcp_metrics": ["network.interface.out.errors.*" ],
	    "pcp_metrics_log": ["network.interface.out.errors" ],
	    "pcp_deriveds": [ "network_tx_errors = rate(network.interface.out.errors)" ],
	    "pcp_predicate": { "label": "network tx errors",
                               "metric": "network_tx_errors",
                               "operator": "threshold_min",
                               "value" : 0.01 },
	    "urls": [ "https://access.redhat.com/solutions/518893" ],
	    "details": "In general the the operation of the network devices should be error free. ",
	    "parents": ["Network TX"]
        },
        {
            "name": "Network RX - errors",
	    "description": "Show network errors",
	    "pcp_metrics": ["network.interface.in.errors.*" ],
	    "pcp_metrics_log": ["network.interface.in.errors"],
	    "pcp_deriveds": [ "network_rx_errors = rate(network.interface.in.errors)" ],
	    "pcp_predicate": { "label": "network rx errors",
                               "metric": "network_rx_errors",
                               "operator": "threshold_min",
                               "value" : 0.01 },
	    "urls": [ "https://access.redhat.com/solutions/518893" ],
	    "details": "In general the the operation of the network devices should be error free. ",
	    "parents": ["Network RX"]
        },
        {
            "name": "Network - RX queue too small",
	    "description": "Per-cpu RX queue are filled to capacity and some RX packet are being dropped as a result",
	    "pcp_metrics": ["network.softnet.dropped"],
	    "pcp_metrics_log": ["network.softnet.dropped"],
	    "pcp_deriveds": [ "rxcpuqdropped = rate(network.softnet.dropped)" ],
	    "pcp_predicate": { "label": "rx cpu queue drop",
                               "metric": "rxcpuqdropped",
                               "operator": "threshold_min",
                               "value" : 0.01 },
	    "urls": [ "https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf",
		      "https://access.redhat.com/solutions/1241943" ],
	    "details": "Each processor in the machine has a queue that stores packets recieved by the NIC but not yet processed by the Linux kernel network stack.  The size of each of the queues is specified by /proc/sys/net/core/netdev_max_backlog.  If a queue has netdev_max_backlog entries in it, any additional packets received from the NIC are dropped rather than added to the full queue.",
	    "parents": ["Network RX"]
        },
        {
            "name": "Network - RX packet processing exceeding time quota",
	    "description": "The RX packet processing function had more work remaining when it ran out of time",
	    "pcp_metrics": ["network.softnet.time_squeeze"],
	    "pcp_metrics_log": ["network.softnet.time_squeeze"],
	    "pcp_deriveds": [ "time_squeeze = rate(network.softnet.time_squeeze)" ],
	    "pcp_predicate": { "label": "exceeded rx time budget",
                               "metric": "time_squeeze",
                               "operator": "threshold_min",
                               "value" : 0.01 },
	    "urls": [ "https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf",
		      "https://access.redhat.com/solutions/1241943" ],
	    "details": "There may be multiple packets waiting to be moved out of the NIC receive ring buffer.  For efficiency the process will attempt to process multiple packets and empty the ring buffer in a single operation.  However, to avoid monopolizing the processor time, preventing other tasks from running, the amount of time that the net_rx_action function is allowed to run is limited.  The net.core.netdev_budget sets an upper limit on how long net_rx_action can run regardless whether there is addition packets to process.  Having to do multiple net_rx_action calls to clear out the receive ring buffer can be less efficient.  Increasing the net.core.netdev_budget could avoid some of the inefficiency",
	    "parents": ["Network RX"]
        }
    ]
}
