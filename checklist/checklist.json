{
    "nodes": [
        {
            ".name": "short title of node",
	    ".description": "more detailed explanation of the issue",
	    ".pcp_metrics": "information from pcp to determine whether problem",
	    ".predicate": "test to determine if problem",
	    ".urls": [ "list of extern URLs (for example kbase articles) for addressing the issue" ],
	    ".parents": ["one or more nodes that would be checked before this node"]
        },
        {
            "name": "Start",
	    "predicate": 1,
            "pcp_metrics": ["kernel.all.load.1 minute"],
            "pcp_metrics_log": ["kernel.all.load"]
        },
        {
            "name": "CPU",
	    "description": "The speed of the CPU is limiting performance",
            "pcp_metrics": ["cpubusy.*"],
            "pcp_metrics_log": ["cpubusy" ],
            "pcp_deriveds": [ "cpubusy = 1 - rate(kernel.percpu.cpu.idle)" ],
            "pcp_predicate": { "label": "processors busy",
                               "metric": "cpubusy",
                               "operator": "threshold_min",
                               "value" : 0.85 },
	    "parents": ["Start"]
        },
        {
            "name": "Storage",
	    "description": "Excessive waiting for storage",
	    "pcp_metrics": ["diskbusy.*"],
	    "pcp_metrics_log": ["diskbusy"],
            "pcp_deriveds": [ "diskbusy = rate(disk.dm.avactive)" ],
            "pcp_predicate": { "label": "disk busy",
                               "metric": "diskbusy",
                               "operator": "threshold_min",
                               "value" : 0.85 },
	    "parents": ["Start"]
        },
        {
            "name": "Storage - bandwidth",
	    "description": "Saturating bandwidth of storage",
	    "pcp_metrics": [ "disk.dm.total.*", "kernel.all.uptime.*" ],
	    "pcp_metrics_log": [ "disk.dm.total", "kernel.all.uptime" ],
	    "predicate": "delta(disk.dm.total)/delta(time)",
	    "parents": ["Storage"]
        },
        {
            "name": "Storage - small blocks",
	    "description": "Excessive waiting for storage",
	    "pcp_metrics": [ "disk.dm.total.*", "disk.dm.blktotal.*"],
	    "pcp_metrics_log": [ "disk.dm.total", "disk.dm.blktotal"],
            "pcp_deriveds": [ "blkrate = delta(disk.dm.total)/delta(disk.dm.blktotal)" ],
	    "pcp_predicate": { "label": "> 64 blocks per something",
                               "metric": "blkrate",
                               "operator": "threshold_min",
                               "value" : 64 },
	    "parents": ["Storage"]
        },
        {
            "name": "Memory",
	    "description": "Running low on free memory",
	    "pcp_metrics": [ "mem.freemem", "mem.physmem" ],
	    "pcp_deriveds": [ "mem_free = mem.freemem/mem.physmem" ],
            "pcp_predicate": { "label": "< 5% free memory",
                               "operator": "threshold_max",
                               "metric": "mem_free",
                               "value": 0.05 },
	    "parents": ["Start"]
        },
        {
            "name": "Memory - swapping",
	    "description": "Not enough physical memory and data being moved between memory and swap space",
	    "pcp_metrics":  [ "swap.pagesin" , "swap.pagesout" ], 
	    "parents": ["Memory"]
        },
        {
            "name": "Memory - low system memory",
	    "description": "Not enough memory in the system resulting in data being moved between memory and storage",
	    "pcp_metrics":  [ "mem.util.free", "mem.physmem" ],
	    "pcp_metrics_log":  [ "mem.util.free", "mem.physmem" ],
	    "predicate": "",
	    "article": "",
	    "parents": ["Memory - swapping"]
        },
        {
            "name": "Memory - low NUMA node memory",
	    "description": "Not enough memory on or more NUMA nodes resulting in data being moved between memory and storage",
	    "pcp_metrics":  [ "mem.numa.util.free.*", "mem.numa.util.total.*" ],
	    "pcp_metrics_log":  [ "mem.numa.util.free.*", "mem.numa.util.total.*" ],
	    "predicate": "",
	    "article": ["https://access.redhat.com/solutions/406253", "https://access.redhat.com/solutions/465463" ],
	    "parents": ["Memory - swapping"]
        },
        {
            "name": "Memory - Low mm reclaim efficency",
	    "description": "The memory management system spending too much effort relaiming memory pages",
	    "pcp_metrics":  [ "mem.vmstat.pgsteal_direct",  "mem.vmstat.pgsteal_kswapd", "mem.vmstat.pgscan_direct",  "mem.vmstat.pgscan_kswapd" ],
	    "pcp_metrics_log":  [ "mem.vmstat.pgsteal_direct",  "mem.vmstat.pgsteal_kswapd", "mem.vmstat.pgscan_direct",  "mem.vmstat.pgscan_kswapd" ],
	    "pcp_deriveds": [ "vmeff = (mem.vmstat.pgsteal_direct+mem.vmstat.pgsteal_kswapd)/(mem.vmstat.pgscan_direct+mem.vmstat.pgscan_kswapd)"],
	    "predicate": { "label":  "< 50% scan recovery rate",
			   "operator": "threshold_max",
			   "metric": "vmeff",
			   "value": 0.50 },
	    "article": "https://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases",
	    "details": "The linux virtual memory management system has a mechanism that places pages in memory into three lists: active, inactive, and free.  When a page is initially used it is put on the active list. Over time pages on the active list may fall off the end of the active list and be added to the inactive list as possible canidates to reclaim and to reuse for other data.  The inactive list is linearly scanned for possible pages to reclaim, but pages on the inactive list may have various reasons to disqualify them from being reclaimed.  This scanning of the inactive list for candidates to reclaim requires cpu processing.  One would like the the computer system to be efficient and avoid having to scan many pages on the inactive list to find few candidates to reclaim. This problem behavior can be observed with a low %vmeff in the 'sar -B' output, a low number of pgsteal/s in relation to the sum of pgscank/s and pgscand/s.",
	    "issues": ["The various aggregate pgsteal and pgscan pcp metrics do not currently exist",
		       "The specific pgsteal and pgscan metrics provided by vmstat vary between kernels"],
	    "parents": ["Start"]
        },
        {
            "name": "Memory - huge page defragmentation",
	    "description": "The system is spending large amounts of time grouping small pages of memory together into contigious physical regions of memory",
	    "pcp_metrics": [ "mem.vmstat.thp_collapse_alloc", "mem.vmstat.thp_fault_alloc", "mem.vmstat.thp_fault_fallback" ],
	    "urls": [ "http://dl.acm.org/citation.cfm?id=2930834" ],
	    "parents": ["Memory"]
        },
        {
	    "name": "Memory - huge page fragmentation",
	    "description": "The system is splitting large regions of memory (Huge pages) into small pages",
	    "pcp_metrics": [ "mem.vmstat.thp_split" ],
	    "urls": [ "https://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases" ],
	    "details": "The Transparent Huge Page (THP) mechanism in some situations can reduce the overhead caused by TLB misses by using a individual Translation Lookaside Buffer (TLB) entry that provide a virtual to physical mapping for a 2MB of region memory rather than requiring 512 TLB entries to providing the virtual to physical memory mapping 2MB of 4096 byte pages.  Thus, more memory can be address with the name number of TLB entries, reducing the number of expensive updates to the TLB to include a virtual to physical mapping that is not currently in the TLB. TLB entries are a very limited resource, ranging from dozens to hundreds of entries. There are cases where the THP mechanism may need to split the huge pages into the smaller pages or combine smallers pages into a larger page.  This can occur when the a huge page is moved between Non-Uniform Memory Access (NUMA) nodes to rebalance memory use (*). Monitoring the Performance Copilot metric mem.vmstat.thp_split (or /proc/vmstat thp_split) would indicate when the expensive splitting of huge pages is occurring.",
	    "issues": ["The RHEL7 has mem.vmstat.thp_split (thp_split in /proc/vmstat is available but on Fedora 25 /proc/vmstat has thp_split_page and thp_split_pmd which do not match up with PCP's mem.vmstat.thp_split"],
	    "parents": ["Memory"]
        },
        {
            "name": "Network",
	    "description": "Amount of network trafic",
	    "pcp_metrics": ["network.interface.in.bytes.*", "network.interface.out.bytes.*" ],
	    "pcp_metrics_log": ["network.interface.out.bytes", "network.interface.in.bytes"],
	    "parents": ["Start"]
        },
        {
            "name": "Network - Saturation",
	    "description": "Network packets being dropped",
	    "pcp_metrics": ["network.interface.out.drops.*", "network.interface.in.drops.*" ],
	    "pcp_metrics_log": ["network.interface.out.drops", "network.interface.in.drops" ],
	    "parents": ["Network"]
        },
        {
            "name": "Network - errors",
	    "description": "Show network errors",
	    "pcp_metrics": ["network.interface.out.errors.*", "network.interface.in.errors.*" ],
	    "pcp_metrics_log": ["network.interface.out.errors", "network.interface.in.errors"],
	    "parents": ["Network"]
        },
        {
            "name": "Network - RX queue too small",
	    "description": "Per-cpu RX queue are filled to capacity and some RX packet are being dropped as a result",
	    "pcp_metrics": ["network.softnet.dropped"],
	    "pcp_metrics_log": ["network.softnet.dropped"],
	    "urls": [ "https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf",
		      "https://access.redhat.com/solutions/1241943" ],
	    "details": "Each processor in the machine has a queue that stores packets recieved by the NIC but not yet processed by the Linux kernel network stack.  The size of each of the queues is specified by /proc/sys/net/core/netdev_max_backlog.  If a queue has netdev_max_backlog entries in it, any additional packets received from the NIC are dropped rather than added to the full queue.",
	    "parents": ["Network"]
        },
        {
            "name": "Network - RX packet processing exceeding time quota",
	    "description": "The RX packet processing function had more work remaining when it ran out of time",
	    "pcp_metrics": ["network.softnet.time_squeeze"],
	    "pcp_metrics_log": ["network.softnet.time_squeeze"],
	    "urls": [ "https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf",
		      "https://access.redhat.com/solutions/1241943" ],
	    "details": "There may be multiple packets waiting to be moved out of the NIC receive ring buffer.  For efficiency the process will attempt to process multiple packets and empty the ring buffer in a single operation.  However, to avoid monopolizing the processor time, preventing other tasks from running, the amount of time that the net_rx_action function is allowed to run is limited.  The net.core.netdev_budget sets an upper limit on how long net_rx_action can run regardless whether there is addition packets to process.  Having to do multiple net_rx_action calls to clear out the receive ring buffer can be less efficient.  Increasing the net.core.netdev_budget could avoid some of the inefficiency",
	    "parents": ["Network"]
        }
    ]
}
